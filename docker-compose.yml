services:
  backend:
    build:
      context: .
      dockerfile: backend/Dockerfile
    depends_on:
      - carla
    mem_limit: "16g"
    ports:
      - "7000:7000"
    volumes:
      - ./:/app:ro
      - D:/Datasets:/Storage:rw
      - ./pipeline_runs:/app/pipeline_runs
    # Request all available GPUs (requires NVIDIA Container Toolkit and CUDA-capable host)
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
    gpus: all
    environment:
      - UVICORN_HOST=0.0.0.0
      - UVICORN_PORT=7000
      - STORAGE_ROOT=/Storage
      - STORAGE_INDEX_PATH=/Storage/storage_index.json
      - LOCALAI_MODEL=gpt-4o-mini-ally
    extra_hosts:
      - "host.docker.internal:host-gateway"

  carla:
    build:
      context: .
      dockerfile: Carla/Dockerfile
    mem_limit: "16g"
    ports:
      - "2000-2002:2000-2002"
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
    gpus: all
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility,graphics
    shm_size: "4g"
    restart: unless-stopped
    command:
      [
        "bash",
        "CarlaUE4.sh",
        "-RenderOffScreen",
        "-nosound",
        "-quality-level=Low",
        "-carla-rpc-port=2000",
        "-carla-streaming-port=0"
      ]

  mlflow:
    image: ghcr.io/mlflow/mlflow:latest
    command: mlflow server --host 0.0.0.0 --port 5000 --backend-store-uri sqlite:///mlflow.db --default-artifact-root /mlflow/artifacts
    ports:
      - "5000:5000"
    volumes:
      - ./mlflow:/mlflow
    environment:
      - MLFLOW_TRACKING_URI=http://0.0.0.0:5000

  web:
    image: nginx:1.27-alpine
    depends_on:
      - backend
    ports:
      - "8000:80"
    volumes:
      - ./WebUI:/usr/share/nginx/html:ro
      - ./:/repo:ro
      - ./nginx.conf:/etc/nginx/conf.d/default.conf:ro
    environment:
      - NGINX_PORT=80

  mcp-server:
    image: ghcr.io/ggerganov/llama.cpp:full
    container_name: mcp-server
    command:
      [
          "--server",
          "-m", "/Storage/LlamaDataset/tinyllama-1.1b-chat-v1.0.F16.gguf",
          "-c", "1024",
          "--port", "8081",
          "--host", "0.0.0.0",
        ]
    volumes:
      - ./models:/models:ro
      - D:/Datasets:/Storage:ro
    ports:
      - "8081:8081"
    restart: unless-stopped

  whisper-api:
    build: ./whisper_api
    container_name: whisper-api
    environment:
      - WHISPER_MODEL=/models/audio/whisper/ggml-base.en.bin
    volumes:
      - ./models:/models:ro
    ports:
      - "8082:8082"
    restart: unless-stopped
